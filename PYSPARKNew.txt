PYSPARK ➡️Day 1️⃣  ➡️ Question:
 PROBLEM STATEMENT : Merge 2 DataFrames
---------------------------------------------------

hashtag#Define the schema of the DataFrame
COLUMNS='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING, MARKS INTEGER'

COLUMNS1='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING'

hashtag#Data to be inserted into the Dataframe
sampleData1=[(1,"ASIF","CSIT","JHARKHAND",95),
 (2,"ARUNESH","ECE","WEST BENGAL",80),
 (3,"HARSHEET","CS","BIHAR",87),
 (4,"SUMBUL","CIVIL","UP",76)]

sampleData2=[(5,"RONALDO","CSIT","DELHI"),
 (6,"MBAPPE","ECE","ASSAM"),
 (7,"GIROUD","CS","GUJRAT"),
 (8,"GAVI","CIVIL","MANIPUT")]

Create and show the DataFrame
-------------------------------------
df1=spark.createDataFrame(sampleData1,COLUMNS)
df1.show
()

df2=spark.createDataFrame(sampleData2,COLUMNS1)
df2.show
()

SOLUTION:
-------------
from pyspark.sql.functions import *

Adding a Column name 'Marks' into df2 DataFrame, so that both the dataframes has equal no. of columns, then we can use UNION

df2=df2.withColumn("MARKS",lit("NULL"))
final=df1.union(df2)
final.show
()

OUTPUT:
----------
+---+--------+----------+-----------+-----+
| ID| NAME|DEPARTMENT| STATE |MARKS|
+---+--------+----------+-----------+-----+
| 1| ASIF | CSIT| JHARKHAND | 95|
| 2| ARUNESH | ECE|WEST BENGAL | 80|
| 3|HARSHEET | CS| BIHAR | 87|
| 4| SUMBUL | CIVIL| UP | 76|
| 5| RONALDO| CSIT| DELHI | NULL|
| 6| MBAPPE | ECE| ASSAM | NULL|
| 7| GIROUD | CS| GUJRAT | NULL|
| 8| GAVI | CIVIL| MANIPUT | NULL|
+---+--------+----------+-----------+-----+

Hey hashtag#DataEngineers

🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 1️⃣ :
================================================
Q) Which of the following Spark config is used to configure the maximum size of an automatically broadcasted DataFrame when performing a join?
=================================================

1) spark.sql.inMemoryColumnarStorage.batchSize

2) spark.sql.adaptive.skewedJoin.enabled

3) spark.sql.autoBroadcastJoinThreshold

4) spark.sql.broadcastTimeout
-------------------------------------------------------------------------------------

Hey #DataEngineers

🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 0️⃣ :
========================================
Question : Find the duplicates records in dataframe.

INPUT:
schema = StructType([
StructField("emp_id", IntegerType(), True),
StructField("emp_name", StringType(), True),
StructField("emp_gender", StringType(), True),
StructField("emp_age", IntegerType(), True),
StructField("emp_salary", IntegerType(), True),
StructField("emp_manager", StringType(), True)
])

DATA= [
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(2, "Aarav Sharma", "Male", 28, 55000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(6, "Naina Verma", "Female", 31, 72000, "Arjun Patel"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(5, "Aditya Kapoor", "Male", 28, 58000, "Zara Singh"),
(10, "Anaya Joshi", "Female", 27, 59000, "Aarav Sharma"),
(11, "Rohan Malhotra", "Male", 36, 73000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel")
]

#Creating a DataFrame->
----------------------------
df = spark.createDataFrame(data, schema=schema)
df.show()

Solution:
from pyspark.sql.types import StructType , StructField , IntegerType , StringType
from pyspark.sql.functions import col
df_group = df.groupBy(df.columns).count()
df_ans = df_group.where(col("count") > 1).drop(col("count"))
df_ans.show()



OUTPUT->
---------------------------------------------------------------------------------------
+------+------------+-------------+---------+-----------+-----------------+
|emp_id| emp_name|emp_gender|emp_age |emp_salary| emp_manager |
+------+------------+-------------+---------+-----------+-----------------+
| 1 |Arjun Patel | Male | 30 | 60000 |Aarav Sharma |
| 3 | Zara Singh | Female | 35 | 70000 | Arjun Patel |
| 4 |Priya Reddy| Female | 32 | 65000 |Aarav Sharma |
+------+------------+-------------+-------+--------------+----------------+

Hey #DataEngineers



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 3️⃣  :

================================================

Q) Solve using Regexp_extract method.

Separate the word whenever the first letter comes after digit and paste it in 2 separate columns.

=================================================



INPUT:

------------------------------------------------------------------------------------

data=[('ABSHFJFJ12QWERT12',1),

      ('QWERT5674OTUT1',2),

      ('DGDGNJDJ1234UYI',3)]

-------------------------------------------------------------------------------------

OUTPUT:

-------------------------------------------------------------------------------------

+---------------+-------------+

|     first_half      |second_half |

+---------------+-------------+

|  ABSHFJFJ12    |    QWERT12|

|   QWERT5674  |      OTUT1   |

|DGDGNJDJ1234|        UYI     |

+------------+----------------+

--------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.functions import regexp_extract



# Create DataFrame with schema

df = spark.createDataFrame(data, schema="input_string string, id int")



#Show the original DataFrame

print("Original DataFrame:")

df.show()



#Apply regular expressions to extract first and second halves

df2 = df.withColumn("first_half", regexp_extract("input_string", "^([a-zA-Z]*([0-9]*))", 1)) \

    .withColumn("second_half", regexp_extract("input_string", "(.)(\d+)(\w+)", 3)) \

    .drop("input_string", "id")



#Show the transformed DataFrame

print("Transformed DataFrame:")

df2.show()



--------------------------------------------

Hey #DataEngineers



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊 :

================================================

Spark Optimization Plan:

--------------------------------------------------------------------------------------

When a query (through SQL/Dataframe) is submitted to spark, it goes through different logical plans before it is actually executed to get results.

EXPLAIN command can be used to print the various or all of the plans that executed during the course of execution.

1) explain() - which by default has value False, which means it will print only the physical plan.

2) explain(True) - it will print all the plans explained below:

3) explain(mode="XXX") : mode is also optional and can take below values:

 simple: Print only a physical plan.

 extended: Print both logical and physical plans.

 codegen: Print a physical plan and generated codes if they are available.

 cost: Print a logical plan and statistics if they are available.

 formatted: Split explain output into two sections: a physical plan outline and node details.



1) Parsed Logical plan (Unresolved):

 in this plan, any syntax errors will be caught

 

2) Analyzed Logical Plan: (resolved)

 in this plan , any errors in the names of the projection objects (columns/tables in select stmts) would be analyzed 

 

3) Optimized Logical Plan:

 there are certain set of pre-defined rules, based on which the plan will be optimized , like:

 i) predicate pushdown -> which pushing the filters down so that we filter the data as early as possible so we have less data for processing

 ii) combine multi projection into one (select stmts)

 iii) combine multi filters into one (where clauses)

 

 ** explain(true) will display details of all the plans executed when the query is submitted 

 == Parsed Logical Plan ==

 All synta errors are checked 

 == Analyzed Logical Plan ==

 shows all the resolved column names and table name (Relation is the output of this plan) 

 == Optimized Logical Plan ==

 

4) Physical Plan: 

 if we are using any aggregation/join in the query, it will decide 

 1) what aggregation strategy to be used (hash/sort aggregate)

 2) what join strategy to be used (broadcast hash join/sort merge join/shuffle hash join) 
There can be multiple Physical plans generated, but only the most cost effective one will be selected and code generated for the same (codeGen), 

which finally gets executed to get the result.


Hey #DataEngineers,



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 2️⃣ 5️⃣  :

================================================

💻 Given a dataset with columns PERSON, TYPE, and AGE,

create an output where the oldest adult is paired with the youngest child, producing pairs of ADULT and CHILD while ensuring appropriate data matching.



💡 Check out the input and output in the table below!



Input:--->



| PERSON | TYPE | AGE |

| ------ | ------ | --- |

| A1 | ADULT | 54 |

| A2 | ADULT | 53 |

| A3 | ADULT | 52 |

| A4 | ADULT | 58 |

| A5 | ADULT | 54 |

| C1 | CHILD | 20 |

| C2 | CHILD | 19 |

| C3 | CHILD | 22 |

| C4 | CHILD | 15 |





Expected Output:--->



| ADULT | CHILD |

| ----- | ----- |

| A4 | C4 |

| A5 | C2 |

| A1 | C1 |

| A2 | C3 |

| A3 | NULL |



import os, sys

from pyspark.sql import SparkSession

from pyspark.sql.functions import col, row_number, desc

from pyspark.sql.window import Window



os.environ['PYSPARK_PYTHON'] = sys.executable

os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

spark = SparkSession.builder.master('local[*]').getOrCreate()



class AdultChildPair:

  def createData(self):

    data = [

      ('A1', 'ADULT', 54),

      ('A2', 'ADULT', 53),

      ('A3', 'ADULT', 52),

      ('A4', 'ADULT', 58),

      ('A5', 'ADULT', 54),

      ('C1', 'CHILD', 20),

      ('C2', 'CHILD', 19),

      ('C3', 'CHILD', 22),

      ('C4', 'CHILD', 15)

    ]

    columns = ['person', 'type', 'age']

    return spark.createDataFrame(data, columns)



 def getPair_pyspark(self, inputDf):

    adultDf = inputDf.filter(col('type').__eq__('ADULT')) \

      .withColumn('rnk', row_number().over(Window.orderBy(desc('age')).orderBy(desc('person'))))



    childDf = inputDf.filter(col('type').__eq__('CHILD'))\

            .withColumn('rnk', row_number().over(Window.orderBy('age').orderBy('person')))



    resultDf = adultDf.alias('A').join(childDf.alias('C'), on='rnk', how='full')\

            .select(col('A.person'),

                col('C.person'))

    return resultDf



ob = AdultChildPair()

inputDf = ob.createData()



resultDf = ob.getPair(inputDf)

resultDf.show()



resultDf = ob.getPair_pyspark(inputDf)

resultDf.show()

---------------------------------------------------------------------------------------

Hey #DataEngineers,



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 2️⃣ 6️⃣  :

================================================

💻 Find the origin and the destination of each customer.

Note: If there is a discontinuity in the flight's origin & destination, it will be a new route (example cust_id 4).

-------------------------------------------------------------------------------------

Input-

cust_id, flight_id, origin, destination

(1, 'Flight2', 'Goa', 'Kochi'),

(1, 'Flight1', 'Delhi', 'Goa'),

(1, 'Flight3', 'Kochi', 'Hyderabad'),

(2, 'Flight1', 'Pune', 'Chennai'),

(2, 'Flight2', 'Chennai', 'Pune'),

(3, 'Flight1', 'Mumbai', 'Bangalore'),

(3, 'Flight2', 'Bangalore', 'Ayodhya'),

(4, 'Flight1', 'Ahmedabad', 'Indore'),

(4, 'Flight2', 'Indore', 'Kolkata'),

(4, 'Flight3', 'Ranchi', 'Delhi'),

(4, 'Flight4', 'Delhi', 'Mumbai')

------------------------------------------------------------------------------------

Output-

|cust_id|  origin|destination|

+-------+---------+-----------+

|   1|  Delhi| Hyderabad|

|   2|   Pune|    Pune|

|   3|  Mumbai|  Ayodhya|

|   4|Ahmedabad|  Kolkata|

|   4|Ranchi |   Mumbai|

--------------------------------------------------------------------------------

In Databricks, you can use PySpark for data processing. Here's how you can achieve the same result using PySpark:

--------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.window import Window

from pyspark.sql.functions import col, lag, when, sum as pyspark_sum



# Create a SparkSession

spark = SparkSession.builder \

  .appName("Customer Route Analysis") \

  .getOrCreate()



# Input data

data = [

  (1, 'Flight2', 'Goa', 'Kochi'),

  (1, 'Flight1', 'Delhi', 'Goa'),

  (1, 'Flight3', 'Kochi', 'Hyderabad'),

  (2, 'Flight1', 'Pune', 'Chennai'),

  (2, 'Flight2', 'Chennai', 'Pune'),

  (3, 'Flight1', 'Mumbai', 'Bangalore'),

  (3, 'Flight2', 'Bangalore', 'Ayodhya'),

  (4, 'Flight1', 'Ahmedabad', 'Indore'),

  (4, 'Flight2', 'Indore', 'Kolkata'),

  (4, 'Flight3', 'Ranchi', 'Delhi'),

  (4, 'Flight4', 'Delhi', 'Mumbai')

]



# Create DataFrame

df = spark.createDataFrame(data, ['cust_id', 'flight_id', 'origin', 'destination'])



# Calculate previous destination

windowSpec = Window.partitionBy('cust_id').orderBy('flight_id')

df = df.withColumn('prev_dest', lag(col('destination')).over(windowSpec))



# Compare origin with previous destination to get new partition column

df = df.withColumn('partition', when(col('origin') != col('prev_dest'), 1).otherwise(0))

df = df.withColumn('partition', pyspark_sum('partition').over(Window.partitionBy('cust_id').orderBy('flight_id')))



# Select first & last value of origin & destination for desired output

output_df = df.groupBy('cust_id', 'partition') \

  .agg({'origin': 'first', 'destination': 'last'}) \

  .orderBy('cust_id', 'partition') \

  .select('cust_id', 'first(origin)', 'last(destination)') \

  .toDF('cust_id', 'origin', 'destination')



output_df.show()

Hey #DataEngineers,



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 2️⃣ 7️⃣  :

================================================

💻 Problem: You have a dataset of sales transactions, and you want to find the top-selling product for each month. Write a PySpark query to achieve this using window functions.



📊Source Data:

Assume you have a PySpark DataFrame named `sales_data` with the following columns:

- `product_id` (String): The unique identifier for each product.

- `sales_date` (Date): The date of the sale.

- `sales_amount` (Double): The amount of the sale.

🔎Here's a sample of the source data:

+-----------+-----------+------------+

| product_id|sales_date |sales_amount|

+-----------+-----------+------------+

| A | 2023-01-15| 100.0 |

| B | 2023-01-20| 150.0 |

| A | 2023-02-10| 120.0 |

| B | 2023-02-15| 180.0 |

| C | 2023-03-05| 200.0 |

| A | 2023-03-10| 250.0 |

+-----------+-----------+------------+





📊Expected Output:

You should generate a result Data Frame that shows the top-selling product for each month. The result should have the following columns:

- `month` (String): The month for which the top-selling product is calculated.

- `top_product` (String): The product that had the highest total sales for that month.



Here's the expected output for the provided sample data:

+-------+------------+

| month|top_product |

+-------+------------+

|2023-01| B |

|2023-02| B |

|2023-03| A |

+-------+------------+



---------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.window import Window

import pyspark.sql.functions as F



# Create a SparkSession

spark = SparkSession.builder.appName("TopSellingProduct").getOrCreate()



# Assuming 'sales_data' is your DataFrame

window_spec = Window.partitionBy(F.year("sales_date"), F.month("sales_date")).orderBy(F.desc("sales_amount"))



result = sales_data.withColumn("rank", F.rank().over(window_spec))



top_selling_products = result.filter(F.col("rank") == 1).select(

 F.date_format("sales_date", "yyyy-MM").alias("month"),

 F.col("product_id").alias("top_product")

)



top_selling_products.show()

--------------------------------------------------------------------------------------

Hey #DataEngineers,



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 2️⃣ 8️⃣  :

================================================

💻 Problem: Question 123: How to read Nested Json files in PySpark?

----------------------------------------------------------------------------------

Let’s dive into the process of reading nested JSON files using PySpark.

-----------------------------------------------------------------------------------

#Step1: Importing Necessary Libraries

First, we need to import the necessary PySpark libraries.



from pyspark.sql import SparkSession

from pyspark.sql.functions import explode



#Step2: Creating a Spark Session

Next, we create a Spark session, which is the entry point to any Spark functionality.



spark = SparkSession.builder.appName('nestedJSON').getOrCreate()



#Step3: Loading the JSON File

Now, we load the JSON file using the read.json function.



df = spark.read.json('path_to_your_json_file')



#Step4: Exploring the Data 

To understand the structure of our data, we use the printSchema() function.



df.printSchema()



#Step5: Flattening the JSON File

To flatten the nested JSON file, we use the explode function. This function creates a new row for each element in the given array or map column.



df_flat = df.select('name', 'age', explode(df.cars).alias('cars'))

df_flat = df_flat.select('name', 'age', 'cars.car1', 'cars.model')



Reading nested JSON files in PySpark can be a bit tricky, but with the right approach, it becomes straightforward. By understanding the structure of your data and using PySpark’s powerful functions, you can easily extract and analyze data from nested JSON files.

--------------------------------------------------------------------------------------
Hey #DataEngineers,



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 2️⃣ 9️⃣  :

================================================

💻 Problem:  To get the below output by using pyspark?

----------------------------------------------------------------------------------

Input:

ID col1  col2

1   A   10

1   B   20

2   A   30

2   B   40



Output: 

ID  A  B

1  10 20

2  30 40

---------------------------------------------------------------------------------------

To achieve the desired output using PySpark, you can use the groupBy and pivot operations. 

---------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.functions import col



#Create a Spark session

spark = SparkSession.builder.appName("example").getOrCreate()



#Create the input DataFrame

data = [(1, 'A', 10),

 (1, 'B', 20),

 (2, 'A', 30),

 (2, 'B', 40)]



columns = ['ID', 'col1', 'col2']

df = spark.createDataFrame(data, columns)



#Pivot the DataFrame

pivoted_df = df.groupBy("ID").pivot("col1").agg({"col2": "first"})



# Show the result

pivoted_df.show()

-------------------------------------------------------------------------------

This code uses the groupBy operation to group the DataFrame by the "ID" column and then uses the pivot operation to pivot the "col1" column. Finally, it uses the agg function to aggregate the values in "col2" using the "first" function.

--------------------------------------------------------------------------------------
