PYSPARK â¡ï¸Day 1ï¸âƒ£  â¡ï¸ Question:
 PROBLEM STATEMENT : Merge 2 DataFrames
---------------------------------------------------

hashtag#Define the schema of the DataFrame
COLUMNS='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING, MARKS INTEGER'

COLUMNS1='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING'

hashtag#Data to be inserted into the Dataframe
sampleData1=[(1,"ASIF","CSIT","JHARKHAND",95),
 (2,"ARUNESH","ECE","WEST BENGAL",80),
 (3,"HARSHEET","CS","BIHAR",87),
 (4,"SUMBUL","CIVIL","UP",76)]

sampleData2=[(5,"RONALDO","CSIT","DELHI"),
 (6,"MBAPPE","ECE","ASSAM"),
 (7,"GIROUD","CS","GUJRAT"),
 (8,"GAVI","CIVIL","MANIPUT")]

Create and show the DataFrame
-------------------------------------
df1=spark.createDataFrame(sampleData1,COLUMNS)
df1.show
()

df2=spark.createDataFrame(sampleData2,COLUMNS1)
df2.show
()

SOLUTION:
-------------
from pyspark.sql.functions import *

Adding a Column name 'Marks' into df2 DataFrame, so that both the dataframes has equal no. of columns, then we can use UNION

df2=df2.withColumn("MARKS",lit("NULL"))
final=df1.union(df2)
final.show
()

OUTPUT:
----------
+---+--------+----------+-----------+-----+
| ID| NAME|DEPARTMENT| STATE |MARKS|
+---+--------+----------+-----------+-----+
| 1| ASIF | CSIT| JHARKHAND | 95|
| 2| ARUNESH | ECE|WEST BENGAL | 80|
| 3|HARSHEET | CS| BIHAR | 87|
| 4| SUMBUL | CIVIL| UP | 76|
| 5| RONALDO| CSIT| DELHI | NULL|
| 6| MBAPPE | ECE| ASSAM | NULL|
| 7| GIROUD | CS| GUJRAT | NULL|
| 8| GAVI | CIVIL| MANIPUT | NULL|
+---+--------+----------+-----------+-----+

Hey hashtag#DataEngineers

ğŸ‡ ğğ˜ğ’ğğ€ğ‘ğŠ ğğ«ğšğœğ­ğ¢ğœğ ğğ«ğ¨ğ›ğ¥ğğ¦ ğŸ“ŠDAY 1ï¸âƒ£ 1ï¸âƒ£ :
================================================
Q) Which of the following Spark config is used to configure the maximum size of an automatically broadcasted DataFrame when performing a join?
=================================================

1) spark.sql.inMemoryColumnarStorage.batchSize

2) spark.sql.adaptive.skewedJoin.enabled

3) spark.sql.autoBroadcastJoinThreshold

4) spark.sql.broadcastTimeout
-------------------------------------------------------------------------------------

Hey #DataEngineers

ğŸ‡ ğğ˜ğ’ğğ€ğ‘ğŠ ğğ«ğšğœğ­ğ¢ğœğ ğğ«ğ¨ğ›ğ¥ğğ¦ ğŸ“ŠDAY 1ï¸âƒ£ 0ï¸âƒ£ :
========================================
Question : Find the duplicates records in dataframe.

INPUT:
schema = StructType([
StructField("emp_id", IntegerType(), True),
StructField("emp_name", StringType(), True),
StructField("emp_gender", StringType(), True),
StructField("emp_age", IntegerType(), True),
StructField("emp_salary", IntegerType(), True),
StructField("emp_manager", StringType(), True)
])

DATA= [
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(2, "Aarav Sharma", "Male", 28, 55000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(6, "Naina Verma", "Female", 31, 72000, "Arjun Patel"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(5, "Aditya Kapoor", "Male", 28, 58000, "Zara Singh"),
(10, "Anaya Joshi", "Female", 27, 59000, "Aarav Sharma"),
(11, "Rohan Malhotra", "Male", 36, 73000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel")
]

#Creating a DataFrame->
----------------------------
df = spark.createDataFrame(data, schema=schema)
df.show()

Solution:
from pyspark.sql.types import StructType , StructField , IntegerType , StringType
from pyspark.sql.functions import col
df_group = df.groupBy(df.columns).count()
df_ans = df_group.where(col("count") > 1).drop(col("count"))
df_ans.show()



OUTPUT->
---------------------------------------------------------------------------------------
+------+------------+-------------+---------+-----------+-----------------+
|emp_id| emp_name|emp_gender|emp_age |emp_salary| emp_manager |
+------+------------+-------------+---------+-----------+-----------------+
| 1 |Arjun Patel | Male | 30 | 60000 |Aarav Sharma |
| 3 | Zara Singh | Female | 35 | 70000 | Arjun Patel |
| 4 |Priya Reddy| Female | 32 | 65000 |Aarav Sharma |
+------+------------+-------------+-------+--------------+----------------+

Hey #DataEngineers



ğŸ‡ ğğ˜ğ’ğğ€ğ‘ğŠ ğğ«ğšğœğ­ğ¢ğœğ ğğ«ğ¨ğ›ğ¥ğğ¦ ğŸ“ŠDAY 1ï¸âƒ£ 3ï¸âƒ£  :

================================================

Q) Solve using Regexp_extract method.

Separate the word whenever the first letter comes after digit and paste it in 2 separate columns.

=================================================



INPUT:

------------------------------------------------------------------------------------

data=[('ABSHFJFJ12QWERT12',1),

      ('QWERT5674OTUT1',2),

      ('DGDGNJDJ1234UYI',3)]

-------------------------------------------------------------------------------------

OUTPUT:

-------------------------------------------------------------------------------------

+---------------+-------------+

|     first_half      |second_half |

+---------------+-------------+

|  ABSHFJFJ12    |    QWERT12|

|   QWERT5674  |      OTUT1   |

|DGDGNJDJ1234|        UYI     |

+------------+----------------+

--------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.functions import regexp_extract



# Create DataFrame with schema

df = spark.createDataFrame(data, schema="input_string string, id int")



#Show the original DataFrame

print("Original DataFrame:")

df.show()



#Apply regular expressions to extract first and second halves

df2 = df.withColumn("first_half", regexp_extract("input_string", "^([a-zA-Z]*([0-9]*))", 1)) \

    .withColumn("second_half", regexp_extract("input_string", "(.)(\d+)(\w+)", 3)) \

    .drop("input_string", "id")



#Show the transformed DataFrame

print("Transformed DataFrame:")

df2.show()



--------------------------------------------

Comment your answer in the comment section.









#DataEngineering #BigData #CodingChallenge #LinkedInLearning #pyspark #databricks #dataanalytics #datascience #technology #creativity #jobinterviews #innovation #education #TechCommunity #pytechnologies #databricks #dataanalytics #dailycoding #data #dataframe #azurecloud

