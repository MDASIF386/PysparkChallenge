PYSPARK ➡️Day 1️⃣  ➡️ Question:
 PROBLEM STATEMENT : Merge 2 DataFrames
---------------------------------------------------

hashtag#Define the schema of the DataFrame
COLUMNS='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING, MARKS INTEGER'

COLUMNS1='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING'

hashtag#Data to be inserted into the Dataframe
sampleData1=[(1,"ASIF","CSIT","JHARKHAND",95),
 (2,"ARUNESH","ECE","WEST BENGAL",80),
 (3,"HARSHEET","CS","BIHAR",87),
 (4,"SUMBUL","CIVIL","UP",76)]

sampleData2=[(5,"RONALDO","CSIT","DELHI"),
 (6,"MBAPPE","ECE","ASSAM"),
 (7,"GIROUD","CS","GUJRAT"),
 (8,"GAVI","CIVIL","MANIPUT")]

Create and show the DataFrame
-------------------------------------
df1=spark.createDataFrame(sampleData1,COLUMNS)
df1.show
()

df2=spark.createDataFrame(sampleData2,COLUMNS1)
df2.show
()

SOLUTION:
-------------
from pyspark.sql.functions import *

Adding a Column name 'Marks' into df2 DataFrame, so that both the dataframes has equal no. of columns, then we can use UNION

df2=df2.withColumn("MARKS",lit("NULL"))
final=df1.union(df2)
final.show
()

OUTPUT:
----------
+---+--------+----------+-----------+-----+
| ID| NAME|DEPARTMENT| STATE |MARKS|
+---+--------+----------+-----------+-----+
| 1| ASIF | CSIT| JHARKHAND | 95|
| 2| ARUNESH | ECE|WEST BENGAL | 80|
| 3|HARSHEET | CS| BIHAR | 87|
| 4| SUMBUL | CIVIL| UP | 76|
| 5| RONALDO| CSIT| DELHI | NULL|
| 6| MBAPPE | ECE| ASSAM | NULL|
| 7| GIROUD | CS| GUJRAT | NULL|
| 8| GAVI | CIVIL| MANIPUT | NULL|
+---+--------+----------+-----------+-----+

Hey hashtag#DataEngineers

🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 1️⃣ :
================================================
Q) Which of the following Spark config is used to configure the maximum size of an automatically broadcasted DataFrame when performing a join?
=================================================

1) spark.sql.inMemoryColumnarStorage.batchSize

2) spark.sql.adaptive.skewedJoin.enabled

3) spark.sql.autoBroadcastJoinThreshold

4) spark.sql.broadcastTimeout
-------------------------------------------------------------------------------------

Hey #DataEngineers

🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 0️⃣ :
========================================
Question : Find the duplicates records in dataframe.

INPUT:
schema = StructType([
StructField("emp_id", IntegerType(), True),
StructField("emp_name", StringType(), True),
StructField("emp_gender", StringType(), True),
StructField("emp_age", IntegerType(), True),
StructField("emp_salary", IntegerType(), True),
StructField("emp_manager", StringType(), True)
])

DATA= [
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(2, "Aarav Sharma", "Male", 28, 55000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(6, "Naina Verma", "Female", 31, 72000, "Arjun Patel"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(5, "Aditya Kapoor", "Male", 28, 58000, "Zara Singh"),
(10, "Anaya Joshi", "Female", 27, 59000, "Aarav Sharma"),
(11, "Rohan Malhotra", "Male", 36, 73000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel")
]

#Creating a DataFrame->
----------------------------
df = spark.createDataFrame(data, schema=schema)
df.show()

Solution:
from pyspark.sql.types import StructType , StructField , IntegerType , StringType
from pyspark.sql.functions import col
df_group = df.groupBy(df.columns).count()
df_ans = df_group.where(col("count") > 1).drop(col("count"))
df_ans.show()



OUTPUT->
---------------------------------------------------------------------------------------
+------+------------+-------------+---------+-----------+-----------------+
|emp_id| emp_name|emp_gender|emp_age |emp_salary| emp_manager |
+------+------------+-------------+---------+-----------+-----------------+
| 1 |Arjun Patel | Male | 30 | 60000 |Aarav Sharma |
| 3 | Zara Singh | Female | 35 | 70000 | Arjun Patel |
| 4 |Priya Reddy| Female | 32 | 65000 |Aarav Sharma |
+------+------------+-------------+-------+--------------+----------------+

Hey #DataEngineers



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 3️⃣  :

================================================

Q) Solve using Regexp_extract method.

Separate the word whenever the first letter comes after digit and paste it in 2 separate columns.

=================================================



INPUT:

------------------------------------------------------------------------------------

data=[('ABSHFJFJ12QWERT12',1),

      ('QWERT5674OTUT1',2),

      ('DGDGNJDJ1234UYI',3)]

-------------------------------------------------------------------------------------

OUTPUT:

-------------------------------------------------------------------------------------

+---------------+-------------+

|     first_half      |second_half |

+---------------+-------------+

|  ABSHFJFJ12    |    QWERT12|

|   QWERT5674  |      OTUT1   |

|DGDGNJDJ1234|        UYI     |

+------------+----------------+

--------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.functions import regexp_extract



# Create DataFrame with schema

df = spark.createDataFrame(data, schema="input_string string, id int")



#Show the original DataFrame

print("Original DataFrame:")

df.show()



#Apply regular expressions to extract first and second halves

df2 = df.withColumn("first_half", regexp_extract("input_string", "^([a-zA-Z]*([0-9]*))", 1)) \

    .withColumn("second_half", regexp_extract("input_string", "(.)(\d+)(\w+)", 3)) \

    .drop("input_string", "id")



#Show the transformed DataFrame

print("Transformed DataFrame:")

df2.show()



--------------------------------------------

Hey #DataEngineers



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊 :

================================================

Spark Optimization Plan:

--------------------------------------------------------------------------------------

When a query (through SQL/Dataframe) is submitted to spark, it goes through different logical plans before it is actually executed to get results.

EXPLAIN command can be used to print the various or all of the plans that executed during the course of execution.

1) explain() - which by default has value False, which means it will print only the physical plan.

2) explain(True) - it will print all the plans explained below:

3) explain(mode="XXX") : mode is also optional and can take below values:

 simple: Print only a physical plan.

 extended: Print both logical and physical plans.

 codegen: Print a physical plan and generated codes if they are available.

 cost: Print a logical plan and statistics if they are available.

 formatted: Split explain output into two sections: a physical plan outline and node details.



1) Parsed Logical plan (Unresolved):

 in this plan, any syntax errors will be caught

 

2) Analyzed Logical Plan: (resolved)

 in this plan , any errors in the names of the projection objects (columns/tables in select stmts) would be analyzed 

 

3) Optimized Logical Plan:

 there are certain set of pre-defined rules, based on which the plan will be optimized , like:

 i) predicate pushdown -> which pushing the filters down so that we filter the data as early as possible so we have less data for processing

 ii) combine multi projection into one (select stmts)

 iii) combine multi filters into one (where clauses)

 

 ** explain(true) will display details of all the plans executed when the query is submitted 

 == Parsed Logical Plan ==

 All synta errors are checked 

 == Analyzed Logical Plan ==

 shows all the resolved column names and table name (Relation is the output of this plan) 

 == Optimized Logical Plan ==

 

4) Physical Plan: 

 if we are using any aggregation/join in the query, it will decide 

 1) what aggregation strategy to be used (hash/sort aggregate)

 2) what join strategy to be used (broadcast hash join/sort merge join/shuffle hash join) 
There can be multiple Physical plans generated, but only the most cost effective one will be selected and code generated for the same (codeGen), 

which finally gets executed to get the result.


Hey #DataEngineers,



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 2️⃣ 5️⃣  :

================================================

💻 Given a dataset with columns PERSON, TYPE, and AGE,

create an output where the oldest adult is paired with the youngest child, producing pairs of ADULT and CHILD while ensuring appropriate data matching.



💡 Check out the input and output in the table below!



Input:--->



| PERSON | TYPE | AGE |

| ------ | ------ | --- |

| A1 | ADULT | 54 |

| A2 | ADULT | 53 |

| A3 | ADULT | 52 |

| A4 | ADULT | 58 |

| A5 | ADULT | 54 |

| C1 | CHILD | 20 |

| C2 | CHILD | 19 |

| C3 | CHILD | 22 |

| C4 | CHILD | 15 |





Expected Output:--->



| ADULT | CHILD |

| ----- | ----- |

| A4 | C4 |

| A5 | C2 |

| A1 | C1 |

| A2 | C3 |

| A3 | NULL |



import os, sys

from pyspark.sql import SparkSession

from pyspark.sql.functions import col, row_number, desc

from pyspark.sql.window import Window



os.environ['PYSPARK_PYTHON'] = sys.executable

os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

spark = SparkSession.builder.master('local[*]').getOrCreate()



class AdultChildPair:

  def createData(self):

    data = [

      ('A1', 'ADULT', 54),

      ('A2', 'ADULT', 53),

      ('A3', 'ADULT', 52),

      ('A4', 'ADULT', 58),

      ('A5', 'ADULT', 54),

      ('C1', 'CHILD', 20),

      ('C2', 'CHILD', 19),

      ('C3', 'CHILD', 22),

      ('C4', 'CHILD', 15)

    ]

    columns = ['person', 'type', 'age']

    return spark.createDataFrame(data, columns)



 def getPair_pyspark(self, inputDf):

    adultDf = inputDf.filter(col('type').__eq__('ADULT')) \

      .withColumn('rnk', row_number().over(Window.orderBy(desc('age')).orderBy(desc('person'))))



    childDf = inputDf.filter(col('type').__eq__('CHILD'))\

            .withColumn('rnk', row_number().over(Window.orderBy('age').orderBy('person')))



    resultDf = adultDf.alias('A').join(childDf.alias('C'), on='rnk', how='full')\

            .select(col('A.person'),

                col('C.person'))

    return resultDf



ob = AdultChildPair()

inputDf = ob.createData()



resultDf = ob.getPair(inputDf)

resultDf.show()



resultDf = ob.getPair_pyspark(inputDf)

resultDf.show()

---------------------------------------------------------------------------------------

Hey #DataEngineers,



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 2️⃣ 6️⃣  :

================================================

💻 Find the origin and the destination of each customer.

Note: If there is a discontinuity in the flight's origin & destination, it will be a new route (example cust_id 4).

-------------------------------------------------------------------------------------

Input-

cust_id, flight_id, origin, destination

(1, 'Flight2', 'Goa', 'Kochi'),

(1, 'Flight1', 'Delhi', 'Goa'),

(1, 'Flight3', 'Kochi', 'Hyderabad'),

(2, 'Flight1', 'Pune', 'Chennai'),

(2, 'Flight2', 'Chennai', 'Pune'),

(3, 'Flight1', 'Mumbai', 'Bangalore'),

(3, 'Flight2', 'Bangalore', 'Ayodhya'),

(4, 'Flight1', 'Ahmedabad', 'Indore'),

(4, 'Flight2', 'Indore', 'Kolkata'),

(4, 'Flight3', 'Ranchi', 'Delhi'),

(4, 'Flight4', 'Delhi', 'Mumbai')

------------------------------------------------------------------------------------

Output-

|cust_id|  origin|destination|

+-------+---------+-----------+

|   1|  Delhi| Hyderabad|

|   2|   Pune|    Pune|

|   3|  Mumbai|  Ayodhya|

|   4|Ahmedabad|  Kolkata|

|   4|Ranchi |   Mumbai|

--------------------------------------------------------------------------------------

In Databricks, you can use PySpark for data processing. Here's how you can achieve the same result using PySpark:

--------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.window import Window

from pyspark.sql.functions import col, lag, when, sum as pyspark_sum



# Create a SparkSession

spark = SparkSession.builder \

  .appName("Customer Route Analysis") \

  .getOrCreate()



# Input data

data = [

  (1, 'Flight2', 'Goa', 'Kochi'),

  (1, 'Flight1', 'Delhi', 'Goa'),

  (1, 'Flight3', 'Kochi', 'Hyderabad'),

  (2, 'Flight1', 'Pune', 'Chennai'),

  (2, 'Flight2', 'Chennai', 'Pune'),

  (3, 'Flight1', 'Mumbai', 'Bangalore'),

  (3, 'Flight2', 'Bangalore', 'Ayodhya'),

  (4, 'Flight1', 'Ahmedabad', 'Indore'),

  (4, 'Flight2', 'Indore', 'Kolkata'),

  (4, 'Flight3', 'Ranchi', 'Delhi'),

  (4, 'Flight4', 'Delhi', 'Mumbai')

]



# Create DataFrame

df = spark.createDataFrame(data, ['cust_id', 'flight_id', 'origin', 'destination'])



# Calculate previous destination

windowSpec = Window.partitionBy('cust_id').orderBy('flight_id')

df = df.withColumn('prev_dest', lag(col('destination')).over(windowSpec))



# Compare origin with previous destination to get new partition column

df = df.withColumn('partition', when(col('origin') != col('prev_dest'), 1).otherwise(0))

df = df.withColumn('partition', pyspark_sum('partition').over(Window.partitionBy('cust_id').orderBy('flight_id')))



# Select first & last value of origin & destination for desired output

output_df = df.groupBy('cust_id', 'partition') \

  .agg({'origin': 'first', 'destination': 'last'}) \

  .orderBy('cust_id', 'partition') \

  .select('cust_id', 'first(origin)', 'last(destination)') \

  .toDF('cust_id', 'origin', 'destination')



output_df.show()

---------------------------------------------------------------------------------------



#DataEngineering #BigData #LinkedInLearning #pyspark #databricks #dataanalytics #datascience #technology #creativity #jobinterviews #innovation #education #TechCommunity #pytechnologies #databricks #dataanalytics #dailycoding #data #dataframe #azurecloud #trendytech #scenario #dataanalytics #interviewseries #dataengineer #technicalskills #problemsolving #dataanalysis #jobseekers #interviewprep #codingchallenge #sqlskills #spark #optimization #speculation #sparkdeveloper #interviewpreparation