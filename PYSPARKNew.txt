PYSPARK ➡️Day 1️⃣  ➡️ Question:
 PROBLEM STATEMENT : Merge 2 DataFrames
---------------------------------------------------

hashtag#Define the schema of the DataFrame
COLUMNS='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING, MARKS INTEGER'

COLUMNS1='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING'

hashtag#Data to be inserted into the Dataframe
sampleData1=[(1,"ASIF","CSIT","JHARKHAND",95),
 (2,"ARUNESH","ECE","WEST BENGAL",80),
 (3,"HARSHEET","CS","BIHAR",87),
 (4,"SUMBUL","CIVIL","UP",76)]

sampleData2=[(5,"RONALDO","CSIT","DELHI"),
 (6,"MBAPPE","ECE","ASSAM"),
 (7,"GIROUD","CS","GUJRAT"),
 (8,"GAVI","CIVIL","MANIPUT")]

Create and show the DataFrame
-------------------------------------
df1=spark.createDataFrame(sampleData1,COLUMNS)
df1.show
()

df2=spark.createDataFrame(sampleData2,COLUMNS1)
df2.show
()

SOLUTION:
-------------
from pyspark.sql.functions import *

Adding a Column name 'Marks' into df2 DataFrame, so that both the dataframes has equal no. of columns, then we can use UNION

df2=df2.withColumn("MARKS",lit("NULL"))
final=df1.union(df2)
final.show
()

OUTPUT:
----------
+---+--------+----------+-----------+-----+
| ID| NAME|DEPARTMENT| STATE |MARKS|
+---+--------+----------+-----------+-----+
| 1| ASIF | CSIT| JHARKHAND | 95|
| 2| ARUNESH | ECE|WEST BENGAL | 80|
| 3|HARSHEET | CS| BIHAR | 87|
| 4| SUMBUL | CIVIL| UP | 76|
| 5| RONALDO| CSIT| DELHI | NULL|
| 6| MBAPPE | ECE| ASSAM | NULL|
| 7| GIROUD | CS| GUJRAT | NULL|
| 8| GAVI | CIVIL| MANIPUT | NULL|
+---+--------+----------+-----------+-----+

Hey hashtag#DataEngineers

🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 1️⃣ :
================================================
Q) Which of the following Spark config is used to configure the maximum size of an automatically broadcasted DataFrame when performing a join?
=================================================

1) spark.sql.inMemoryColumnarStorage.batchSize

2) spark.sql.adaptive.skewedJoin.enabled

3) spark.sql.autoBroadcastJoinThreshold

4) spark.sql.broadcastTimeout
-------------------------------------------------------------------------------------

Hey #DataEngineers

🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 0️⃣ :
========================================
Question : Find the duplicates records in dataframe.

INPUT:
schema = StructType([
StructField("emp_id", IntegerType(), True),
StructField("emp_name", StringType(), True),
StructField("emp_gender", StringType(), True),
StructField("emp_age", IntegerType(), True),
StructField("emp_salary", IntegerType(), True),
StructField("emp_manager", StringType(), True)
])

DATA= [
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(2, "Aarav Sharma", "Male", 28, 55000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(6, "Naina Verma", "Female", 31, 72000, "Arjun Patel"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(5, "Aditya Kapoor", "Male", 28, 58000, "Zara Singh"),
(10, "Anaya Joshi", "Female", 27, 59000, "Aarav Sharma"),
(11, "Rohan Malhotra", "Male", 36, 73000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel")
]

#Creating a DataFrame->
----------------------------
df = spark.createDataFrame(data, schema=schema)
df.show()

Solution:
from pyspark.sql.types import StructType , StructField , IntegerType , StringType
from pyspark.sql.functions import col
df_group = df.groupBy(df.columns).count()
df_ans = df_group.where(col("count") > 1).drop(col("count"))
df_ans.show()



OUTPUT->
---------------------------------------------------------------------------------------
+------+------------+-------------+---------+-----------+-----------------+
|emp_id| emp_name|emp_gender|emp_age |emp_salary| emp_manager |
+------+------------+-------------+---------+-----------+-----------------+
| 1 |Arjun Patel | Male | 30 | 60000 |Aarav Sharma |
| 3 | Zara Singh | Female | 35 | 70000 | Arjun Patel |
| 4 |Priya Reddy| Female | 32 | 65000 |Aarav Sharma |
+------+------------+-------------+-------+--------------+----------------+

Hey #DataEngineers



🎇 𝐏𝐘𝐒𝐏𝐀𝐑𝐊 𝐏𝐫𝐚𝐜𝐭𝐢𝐜𝐞 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 📊DAY 1️⃣ 3️⃣  :

================================================

Q) Solve using Regexp_extract method.

Separate the word whenever the first letter comes after digit and paste it in 2 separate columns.

=================================================



INPUT:

------------------------------------------------------------------------------------

data=[('ABSHFJFJ12QWERT12',1),

      ('QWERT5674OTUT1',2),

      ('DGDGNJDJ1234UYI',3)]

-------------------------------------------------------------------------------------

OUTPUT:

-------------------------------------------------------------------------------------

+---------------+-------------+

|     first_half      |second_half |

+---------------+-------------+

|  ABSHFJFJ12    |    QWERT12|

|   QWERT5674  |      OTUT1   |

|DGDGNJDJ1234|        UYI     |

+------------+----------------+

--------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.functions import regexp_extract



# Create DataFrame with schema

df = spark.createDataFrame(data, schema="input_string string, id int")



#Show the original DataFrame

print("Original DataFrame:")

df.show()



#Apply regular expressions to extract first and second halves

df2 = df.withColumn("first_half", regexp_extract("input_string", "^([a-zA-Z]*([0-9]*))", 1)) \

    .withColumn("second_half", regexp_extract("input_string", "(.)(\d+)(\w+)", 3)) \

    .drop("input_string", "id")



#Show the transformed DataFrame

print("Transformed DataFrame:")

df2.show()



--------------------------------------------

Comment your answer in the comment section.









#DataEngineering #BigData #CodingChallenge #LinkedInLearning #pyspark #databricks #dataanalytics #datascience #technology #creativity #jobinterviews #innovation #education #TechCommunity #pytechnologies #databricks #dataanalytics #dailycoding #data #dataframe #azurecloud

