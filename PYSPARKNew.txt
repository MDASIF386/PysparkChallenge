PYSPARK âž¡ï¸Day 1ï¸âƒ£  âž¡ï¸ Question:
 PROBLEM STATEMENT : Merge 2 DataFrames
---------------------------------------------------

hashtag#Define the schema of the DataFrame
COLUMNS='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING, MARKS INTEGER'

COLUMNS1='ID INTEGER, NAME STRING, DEPARTMENT STRING, STATE STRING'

hashtag#Data to be inserted into the Dataframe
sampleData1=[(1,"ASIF","CSIT","JHARKHAND",95),
 (2,"ARUNESH","ECE","WEST BENGAL",80),
 (3,"HARSHEET","CS","BIHAR",87),
 (4,"SUMBUL","CIVIL","UP",76)]

sampleData2=[(5,"RONALDO","CSIT","DELHI"),
 (6,"MBAPPE","ECE","ASSAM"),
 (7,"GIROUD","CS","GUJRAT"),
 (8,"GAVI","CIVIL","MANIPUT")]

Create and show the DataFrame
-------------------------------------
df1=spark.createDataFrame(sampleData1,COLUMNS)
df1.show
()

df2=spark.createDataFrame(sampleData2,COLUMNS1)
df2.show
()

SOLUTION:
-------------
from pyspark.sql.functions import *

Adding a Column name 'Marks' into df2 DataFrame, so that both the dataframes has equal no. of columns, then we can use UNION

df2=df2.withColumn("MARKS",lit("NULL"))
final=df1.union(df2)
final.show
()

OUTPUT:
----------
+---+--------+----------+-----------+-----+
| ID| NAME|DEPARTMENT| STATE |MARKS|
+---+--------+----------+-----------+-----+
| 1| ASIF | CSIT| JHARKHAND | 95|
| 2| ARUNESH | ECE|WEST BENGAL | 80|
| 3|HARSHEET | CS| BIHAR | 87|
| 4| SUMBUL | CIVIL| UP | 76|
| 5| RONALDO| CSIT| DELHI | NULL|
| 6| MBAPPE | ECE| ASSAM | NULL|
| 7| GIROUD | CS| GUJRAT | NULL|
| 8| GAVI | CIVIL| MANIPUT | NULL|
+---+--------+----------+-----------+-----+

Hey hashtag#DataEngineers

ðŸŽ‡ ðð˜ð’ðð€ð‘ðŠ ðð«ðšðœð­ð¢ðœðž ðð«ð¨ð›ð¥ðžð¦ ðŸ“ŠDAY 1ï¸âƒ£ 1ï¸âƒ£ :
================================================
Q) Which of the following Spark config is used to configure the maximum size of an automatically broadcasted DataFrame when performing a join?
=================================================

1) spark.sql.inMemoryColumnarStorage.batchSize

2) spark.sql.adaptive.skewedJoin.enabled

3) spark.sql.autoBroadcastJoinThreshold

4) spark.sql.broadcastTimeout
-------------------------------------------------------------------------------------

Hey #DataEngineers

ðŸŽ‡ ðð˜ð’ðð€ð‘ðŠ ðð«ðšðœð­ð¢ðœðž ðð«ð¨ð›ð¥ðžð¦ ðŸ“ŠDAY 1ï¸âƒ£ 0ï¸âƒ£ :
========================================
Question : Find the duplicates records in dataframe.

INPUT:
schema = StructType([
StructField("emp_id", IntegerType(), True),
StructField("emp_name", StringType(), True),
StructField("emp_gender", StringType(), True),
StructField("emp_age", IntegerType(), True),
StructField("emp_salary", IntegerType(), True),
StructField("emp_manager", StringType(), True)
])

DATA= [
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(2, "Aarav Sharma", "Male", 28, 55000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(6, "Naina Verma", "Female", 31, 72000, "Arjun Patel"),
(1, "Arjun Patel", "Male", 30, 60000, "Aarav Sharma"),
(4, "Priya Reddy", "Female", 32, 65000, "Aarav Sharma"),
(5, "Aditya Kapoor", "Male", 28, 58000, "Zara Singh"),
(10, "Anaya Joshi", "Female", 27, 59000, "Aarav Sharma"),
(11, "Rohan Malhotra", "Male", 36, 73000, "Zara Singh"),
(3, "Zara Singh", "Female", 35, 70000, "Arjun Patel")
]

#Creating a DataFrame->
----------------------------
df = spark.createDataFrame(data, schema=schema)
df.show()

Solution:
from pyspark.sql.types import StructType , StructField , IntegerType , StringType
from pyspark.sql.functions import col
df_group = df.groupBy(df.columns).count()
df_ans = df_group.where(col("count") > 1).drop(col("count"))
df_ans.show()



OUTPUT->
---------------------------------------------------------------------------------------
+------+------------+-------------+---------+-----------+-----------------+
|emp_id| emp_name|emp_gender|emp_age |emp_salary| emp_manager |
+------+------------+-------------+---------+-----------+-----------------+
| 1 |Arjun Patel | Male | 30 | 60000 |Aarav Sharma |
| 3 | Zara Singh | Female | 35 | 70000 | Arjun Patel |
| 4 |Priya Reddy| Female | 32 | 65000 |Aarav Sharma |
+------+------------+-------------+-------+--------------+----------------+

Hey #DataEngineers



ðŸŽ‡ ðð˜ð’ðð€ð‘ðŠ ðð«ðšðœð­ð¢ðœðž ðð«ð¨ð›ð¥ðžð¦ ðŸ“ŠDAY 1ï¸âƒ£ 3ï¸âƒ£  :

================================================

Q) Solve using Regexp_extract method.

Separate the word whenever the first letter comes after digit and paste it in 2 separate columns.

=================================================



INPUT:

------------------------------------------------------------------------------------

data=[('ABSHFJFJ12QWERT12',1),

      ('QWERT5674OTUT1',2),

      ('DGDGNJDJ1234UYI',3)]

-------------------------------------------------------------------------------------

OUTPUT:

-------------------------------------------------------------------------------------

+---------------+-------------+

|     first_half      |second_half |

+---------------+-------------+

|  ABSHFJFJ12    |    QWERT12|

|   QWERT5674  |      OTUT1   |

|DGDGNJDJ1234|        UYI     |

+------------+----------------+

--------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

from pyspark.sql.functions import regexp_extract



# Create DataFrame with schema

df = spark.createDataFrame(data, schema="input_string string, id int")



#Show the original DataFrame

print("Original DataFrame:")

df.show()



#Apply regular expressions to extract first and second halves

df2 = df.withColumn("first_half", regexp_extract("input_string", "^([a-zA-Z]*([0-9]*))", 1)) \

    .withColumn("second_half", regexp_extract("input_string", "(.)(\d+)(\w+)", 3)) \

    .drop("input_string", "id")



#Show the transformed DataFrame

print("Transformed DataFrame:")

df2.show()



--------------------------------------------

Hey #DataEngineers



ðŸŽ‡ ðð˜ð’ðð€ð‘ðŠ ðð«ðšðœð­ð¢ðœðž ðð«ð¨ð›ð¥ðžð¦ ðŸ“ŠDAY 1ï¸âƒ£ 7ï¸âƒ£  :

================================================

Spark Optimization Plan:

--------------------------------------------------------------------------------------

When a query (through SQL/Dataframe) is submitted to spark, it goes through different logical plans before it is actually executed to get results.

EXPLAIN command can be used to print the various or all of the plans that executed during the course of execution.

1) explain() - which by default has value False, which means it will print only the physical plan.

2) explain(True) - it will print all the plans explained below:

3) explain(mode="XXX") : mode is also optional and can take below values:

 simple: Print only a physical plan.

 extended: Print both logical and physical plans.

 codegen: Print a physical plan and generated codes if they are available.

 cost: Print a logical plan and statistics if they are available.

 formatted: Split explain output into two sections: a physical plan outline and node details.



1) Parsed Logical plan (Unresolved):

 in this plan, any syntax errors will be caught

 

2) Analyzed Logical Plan: (resolved)

 in this plan , any errors in the names of the projection objects (columns/tables in select stmts) would be analyzed 

 

3) Optimized Logical Plan:

 there are certain set of pre-defined rules, based on which the plan will be optimized , like:

 i) predicate pushdown -> which pushing the filters down so that we filter the data as early as possible so we have less data for processing

 ii) combine multi projection into one (select stmts)

 iii) combine multi filters into one (where clauses)

 

 ** explain(true) will display details of all the plans executed when the query is submitted 

 == Parsed Logical Plan ==

 All synta errors are checked 

 == Analyzed Logical Plan ==

 shows all the resolved column names and table name (Relation is the output of this plan) 

 == Optimized Logical Plan ==

 

4) Physical Plan: 

 if we are using any aggregation/join in the query, it will decide 

 1) what aggregation strategy to be used (hash/sort aggregate)

 2) what join strategy to be used (broadcast hash join/sort merge join/shuffle hash join) 

There can be multiple Physical plans generated, but only the most cost effective one will be selected and code generated for the same (codeGen), 

which finally gets executed to get the result.



#DataEngineering #BigData #CodingChallenge #LinkedInLearning #pyspark #databricks #dataanalytics #datascience #technology #creativity #jobinterviews #innovation #education #TechCommunity #pytechnologies #databricks #dataanalytics #dailycoding #data #dataframe #azurecloud #trendytech #scenario #dataanalytics #interviewseries #dataengineering #technicalskills #problemsolving #dataanalysis #jobseekers #interviewprep #codingchallenge  #sqlskills #spark #optimization #speculation #sparkdeveloper #interviewpreparation #interviewquestions #interviewtips